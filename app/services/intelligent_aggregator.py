import json
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func
from app.database import DataPoint, Dataset
from app.services.llm_service import LLMService

class IntelligentAggregator:
    """Adaptive multi-stage pipeline for intelligent data aggregation"""
    
    def __init__(self):
        self.llm_service = LLMService()
        self.analysis_context = {}
    
    async def analyze_and_plan(self, county: str, db: AsyncSession, analysis_goal: str = "identify_community_problems") -> Dict[str, Any]:
        """STAGE 1: LLM-Powered Strategy Generation"""
        
        # Profile all datasets for this county
        profiles = await self._profile_datasets(county, db)
        
        # Get existing data characteristics
        data_characteristics = await self._analyze_data_characteristics(county, db)
        
        # Ask LLM for aggregation strategy
        strategy_prompt = f"""
        I have these datasets for {county}:
        {json.dumps(profiles, indent=2)}
        
        Data characteristics:
        {json.dumps(data_characteristics, indent=2)}
        
        My analysis goal: {analysis_goal}
        
        Generate an intelligent aggregation strategy that:
        1. Prioritizes recent, relevant data (last 5-10 years for crime data, last 2-3 years for demographics)
        2. Reduces dimensionality while preserving insights
        3. Identifies key patterns and outliers
        4. Adapts to data density (sparse areas get rolled up)
        5. Focuses on high-variance, high-signal patterns
        6. Considers domain-specific logic (arrest trends vs demographic changes)
        
        Return JSON with specific rules per dataset:
        {{
            "time_focus": {{
                "arrest_data": "recent_5_years",
                "demographics": "recent_3_years",
                "311_complaints": "recent_2_years"
            }},
            "aggregation_rules": {{
                "high_volume_metrics": "sum_by_year",
                "low_volume_metrics": "average_by_period",
                "demographic_metrics": "latest_values"
            }},
            "relevance_filters": {{
                "min_data_points": 10,
                "min_variance_threshold": 0.1,
                "focus_on_outliers": true
            }},
            "analysis_priorities": ["public_safety", "economic_indicators", "demographic_changes"]
        }}
        """
        
        try:
            # Use LLM to generate strategy
            strategy = await self.llm_service._generate_aggregation_strategy(strategy_prompt)
            return strategy
        except:
            # Fallback to default strategy
            return self._get_default_strategy()
    
    async def execute_strategy(self, county: str, strategy: Dict[str, Any], db: AsyncSession) -> Dict[str, Any]:
        """STAGE 2: Automated Profiling & Adjustment"""
        
        # Get all data points for the county
        result = await db.execute(
            select(DataPoint)
            .where(DataPoint.county == county)
        )
        data_points = result.scalars().all()
        
        if not data_points:
            return {}
        
        # Apply intelligent time filtering
        filtered_points = self._apply_time_focus_filtering(data_points, strategy)
        
        # Run aggregations based on strategy
        aggregated = {}
        
        # Group by metric and apply domain-specific logic
        metric_groups = {}
        for point in filtered_points:
            metric = point.metric_name
            if metric not in metric_groups:
                metric_groups[metric] = []
            metric_groups[metric].append(point)
        
        # Apply aggregation rules
        for metric, points in metric_groups.items():
            analysis = await self._apply_aggregation_rules(metric, points, strategy)
            if analysis:
                aggregated[metric] = analysis
        
        # Measure information density and auto-adjust
        adjusted_aggregated = self._auto_adjust_based_on_density(aggregated, strategy)
        
        return adjusted_aggregated
    
    async def iterative_analysis(self, county: str, aggregated_data: Dict[str, Any], db: AsyncSession) -> Dict[str, Any]:
        """STAGE 3: Iterative Refinement"""
        
        # Pass aggregated data to LLM for analysis
        analysis_prompt = f"""
        Based on this aggregated data for {county}:
        {json.dumps(aggregated_data, indent=2)}
        
        Analyze the patterns and identify:
        1. Key insights and anomalies
        2. Areas that need more detailed analysis
        3. Specific drill-downs that would be valuable
        4. Missing context that would improve understanding
        
        If you need more specific data, request it in this format:
        {{
            "drill_down_requests": [
                {{
                    "metric": "violent_crime_arrests",
                    "time_period": "2020-2024",
                    "reason": "High variance detected, need recent trend analysis"
                }}
            ],
            "insights": ["list of key findings"],
            "confidence_level": "high|medium|low"
        }}
        """
        
        try:
            # Get LLM analysis and requests
            llm_response = await self.llm_service._analyze_aggregated_data(analysis_prompt)
            
            # Execute drill-down requests
            if 'drill_down_requests' in llm_response:
                for request in llm_response['drill_down_requests']:
                    drill_down_data = await self._execute_drill_down(county, request, db)
                    aggregated_data[f"{request['metric']}_detailed"] = drill_down_data
            
            return {
                'aggregated_data': aggregated_data,
                'llm_insights': llm_response.get('insights', []),
                'confidence_level': llm_response.get('confidence_level', 'medium'),
                'analysis_timestamp': datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            print(f"LLM analysis failed: {e}")
            # Fallback: return basic aggregated data without LLM insights
            return {
                'aggregated_data': aggregated_data,
                'llm_insights': ["Data analysis completed with basic aggregation"],
                'confidence_level': 'low',
                'analysis_timestamp': datetime.utcnow().isoformat(),
                'fallback_mode': True
            }
    
    async def insight_driven_reaggregation(self, county: str, insights: List[str], db: AsyncSession) -> Dict[str, Any]:
        """STAGE 4: Insight-Driven Re-aggregation"""
        
        # Based on insights, perform targeted re-aggregations
        targeted_analysis = {}
        
        for insight in insights:
            if "arrest" in insight.lower():
                # Focus on crime patterns
                crime_analysis = await self._analyze_crime_patterns(county, db)
                targeted_analysis['crime_patterns'] = crime_analysis
            
            elif "demographic" in insight.lower():
                # Focus on demographic changes
                demo_analysis = await self._analyze_demographic_changes(county, db)
                targeted_analysis['demographic_changes'] = demo_analysis
            
            elif "economic" in insight.lower():
                # Focus on economic indicators
                economic_analysis = await self._analyze_economic_indicators(county, db)
                targeted_analysis['economic_indicators'] = economic_analysis
        
        return targeted_analysis
    
    # Helper Methods
    
    async def _profile_datasets(self, county: str, db: AsyncSession) -> Dict[str, Any]:
        """Profile all datasets for a county"""
        result = await db.execute(
            select(DataPoint)
            .where(DataPoint.county == county)
        )
        data_points = result.scalars().all()
        
        profiles = {}
        for point in data_points:
            category = point.category
            if category not in profiles:
                profiles[category] = {
                    'metrics': set(),
                    'years': set(),
                    'total_points': 0
                }
            
            profiles[category]['metrics'].add(point.metric_name)
            profiles[category]['years'].add(point.year)
            profiles[category]['total_points'] += 1
        
        # Convert sets to lists for JSON serialization
        for category in profiles:
            profiles[category]['metrics'] = list(profiles[category]['metrics'])
            profiles[category]['years'] = list(profiles[category]['years'])
        
        return profiles
    
    async def _analyze_data_characteristics(self, county: str, db: AsyncSession) -> Dict[str, Any]:
        """Analyze data characteristics for intelligent processing"""
        result = await db.execute(
            select(DataPoint)
            .where(DataPoint.county == county)
        )
        data_points = result.scalars().all()
        
        if not data_points:
            return {}
        
        # Calculate basic statistics
        years = [p.year for p in data_points if p.year]
        values = [p.metric_value for p in data_points]
        
        return {
            'total_data_points': len(data_points),
            'year_range': f"{min(years)}-{max(years)}" if years else "unknown",
            'data_density': len(data_points) / len(set(years)) if years else 0,
            'value_statistics': {
                'mean': np.mean(values),
                'std': np.std(values),
                'min': np.min(values),
                'max': np.max(values)
            },
            'categories': list(set(p.category for p in data_points))
        }
    
    def _apply_time_focus_filtering(self, data_points: List[DataPoint], strategy: Dict[str, Any]) -> List[DataPoint]:
        """Apply intelligent time filtering based on strategy"""
        current_year = datetime.now().year
        time_focus = strategy.get('time_focus', {})
        
        filtered_points = []
        
        for point in data_points:
            if not point.year:
                continue
                
            # Determine focus period based on data type
            if point.category == 'crime' or 'arrest' in point.metric_name.lower():
                focus_years = 5  # Last 5 years for crime data
            elif point.category == 'demographics':
                focus_years = 3  # Last 3 years for demographics
            else:
                focus_years = 2  # Last 2 years for other data
            
            if point.year >= (current_year - focus_years):
                filtered_points.append(point)
        
        return filtered_points
    
    async def _apply_aggregation_rules(self, metric: str, points: List[DataPoint], strategy: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Apply domain-specific aggregation rules"""
        if len(points) < 5:  # Skip metrics with too little data
            return None
        
        values = [p.metric_value for p in points]
        years = [p.year for p in points if p.year]
        
        # Calculate metrics based on data characteristics
        total = sum(values)
        avg_per_year = total / len(set(years)) if years else 0
        
        # Recent average (last 3 years)
        recent_years = [y for y in years if y >= (datetime.now().year - 3)]
        recent_values = [p.metric_value for p in points if p.year in recent_years]
        recent_avg = sum(recent_values) / len(recent_values) if recent_values else 0
        
        # Trend analysis
        trend = self._calculate_trend(values, years)
        
        # Anomaly score
        anomaly_score = self._calculate_anomaly_score(total, len(points))
        
        # Relevance multiplier based on recency and variance
        relevance_multiplier = self._calculate_relevance_multiplier(values, years)
        
        return {
            'total': total,
            'average_per_year': avg_per_year,
            'recent_average': recent_avg,
            'trend': trend,
            'anomaly_score': anomaly_score,
            'relevance_multiplier': relevance_multiplier,
            'data_points': len(points),
            'time_period': f"{min(years)}-{max(years)}" if years else "unknown"
        }
    
    def _auto_adjust_based_on_density(self, aggregated: Dict[str, Any], strategy: Dict[str, Any]) -> Dict[str, Any]:
        """Auto-adjust aggregation based on information density"""
        min_data_points = strategy.get('relevance_filters', {}).get('min_data_points', 10)
        min_variance = strategy.get('relevance_filters', {}).get('min_variance_threshold', 0.1)
        
        adjusted = {}
        for metric, data in aggregated.items():
            # Filter out low-density metrics
            if data['data_points'] < min_data_points:
                continue
            
            # Boost metrics with high variance (more interesting)
            if data.get('anomaly_score', 0) > min_variance:
                data['relevance_multiplier'] *= 1.5
            
            adjusted[metric] = data
        
        return adjusted
    
    def _calculate_trend(self, values: List[float], years: List[int]) -> str:
        """Calculate trend direction"""
        if len(values) < 2 or len(set(years)) < 2:
            return "insufficient_data"
        
        # Group by year
        year_totals = {}
        for year, value in zip(years, values):
            if year not in year_totals:
                year_totals[year] = []
            year_totals[year].append(value)
        
        # Calculate averages
        year_averages = {year: sum(vals)/len(vals) for year, vals in year_totals.items()}
        sorted_years = sorted(year_averages.keys())
        
        if len(sorted_years) < 2:
            return "insufficient_data"
        
        first_avg = year_averages[sorted_years[0]]
        last_avg = year_averages[sorted_years[-1]]
        
        change_percent = ((last_avg - first_avg) / first_avg * 100) if first_avg > 0 else 0
        
        if change_percent > 15:
            return "strongly_increasing"
        elif change_percent > 5:
            return "increasing"
        elif change_percent < -15:
            return "strongly_decreasing"
        elif change_percent < -5:
            return "decreasing"
        else:
            return "stable"
    
    def _calculate_anomaly_score(self, total: float, count: int) -> float:
        """Calculate anomaly score (0-1)"""
        if count == 0:
            return 0
        
        avg_per_point = total / count
        
        # Normalize based on magnitude
        if avg_per_point > 1000:
            return 1.0
        elif avg_per_point > 500:
            return 0.8
        elif avg_per_point > 100:
            return 0.6
        elif avg_per_point > 50:
            return 0.4
        else:
            return 0.2
    
    def _calculate_relevance_multiplier(self, values: List[float], years: List[int]) -> float:
        """Calculate relevance multiplier based on recency and variance"""
        if not values or not years:
            return 0.5
        
        # Recency factor (more recent = higher relevance)
        current_year = datetime.now().year
        recent_years = [y for y in years if y >= (current_year - 3)]
        recency_factor = len(recent_years) / len(set(years)) if years else 0.5
        
        # Variance factor (higher variance = more interesting)
        variance_factor = np.std(values) / np.mean(values) if np.mean(values) > 0 else 0
        
        # Combine factors
        relevance = (recency_factor * 0.6) + (min(variance_factor, 1.0) * 0.4)
        return max(0.1, min(2.0, relevance))  # Clamp between 0.1 and 2.0
    
    def _get_default_strategy(self) -> Dict[str, Any]:
        """Fallback default strategy"""
        return {
            "time_focus": {
                "arrest_data": "recent_5_years",
                "demographics": "recent_3_years",
                "311_complaints": "recent_2_years"
            },
            "aggregation_rules": {
                "high_volume_metrics": "sum_by_year",
                "low_volume_metrics": "average_by_period"
            },
            "relevance_filters": {
                "min_data_points": 10,
                "min_variance_threshold": 0.1,
                "focus_on_outliers": True
            },
            "analysis_priorities": ["public_safety", "demographic_changes"]
        }
    
    async def _execute_drill_down(self, county: str, request: Dict[str, Any], db: AsyncSession) -> Dict[str, Any]:
        """Execute specific drill-down analysis"""
        # Implementation for targeted analysis based on LLM requests
        # This would perform specific aggregations based on the request
        return {"drill_down": "implemented", "request": request}
    
    async def _analyze_crime_patterns(self, county: str, db: AsyncSession) -> Dict[str, Any]:
        """Analyze crime patterns specifically"""
        # Implementation for crime-specific analysis
        return {"crime_analysis": "implemented"}
    
    async def _analyze_demographic_changes(self, county: str, db: AsyncSession) -> Dict[str, Any]:
        """Analyze demographic changes specifically"""
        # Implementation for demographic analysis
        return {"demographic_analysis": "implemented"}
    
    async def _analyze_economic_indicators(self, county: str, db: AsyncSession) -> Dict[str, Any]:
        """Analyze economic indicators specifically"""
        # Implementation for economic analysis
        return {"economic_analysis": "implemented"}
